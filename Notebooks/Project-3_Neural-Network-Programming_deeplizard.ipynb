{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a model part 2 - loss, gradient, backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now let's move on to actually training data. \n",
    "\n",
    "# The training process:\n",
    "# 1) Get batch from training set\n",
    "# 2) Pass batch to network\n",
    "# 3) Calculate the loss\n",
    "# 4) Get the gradient of loss vs network weights\n",
    "# 5) Update the weights according to this gradient to reduce loss. \n",
    "\n",
    "# 6) Repeat 1-5 for one epoch (one pass through full training set)\n",
    "# 7) Complete 1-6 for appropriate number of epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x121405e10>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We already know steps 1 and 2. What about the loss function? \n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import torchvision # computer vision package \n",
    "import torchvision.transforms as transforms # interface for common IP transforms. \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms \n",
    "torch.set_printoptions(linewidth=120)\n",
    "torch.set_grad_enabled(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# alright. Back to implementing the network. \n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 6, kernel_size = 5)\n",
    "        self.conv2 = nn.Conv2d(in_channels = 6, out_channels = 12, kernel_size = 5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features = 12*4*4, out_features = 120) # \"fully connected\". Also, \"dense\"\n",
    "        self.fc2 = nn.Linear(in_features = 120, out_features = 60)\n",
    "        self.out = nn.Linear(in_features = 60, out_features = 10)\n",
    "        \n",
    "    def forward(self, t):\n",
    "        # time to actually implement forward, layer by layer\n",
    "        # 1) input layer\n",
    "        t = t\n",
    "        \n",
    "        # 2) conv1 - 3 operations. Convolution, activation, and pooling. \n",
    "        t = self.conv1(t) # that easy - we just call the layer obejct on our input! Neat. \n",
    "        t = F.relu(t) # activation function\n",
    "        t = F.max_pool2d(t, kernel_size = 2, stride = 2) # pool the convolution output \n",
    "        \n",
    "        # 3) conv2\n",
    "        t = self.conv2(t) # that easy - we just call the layer obejct on our input! Neat. \n",
    "        t = F.relu(t) \n",
    "        t = F.max_pool2d(t, kernel_size = 2, stride = 2)\n",
    "        \n",
    "        # 4) fc1\n",
    "        t = t.reshape(-1, 12 * 4 * 4) # when we swap from conv to linear, we need to reshape\n",
    "        # 12 is number of output channels from previous layer\n",
    "        # 4x4 is height and with of each of the channels, determined by previous pooling operations. \n",
    "        t = self.fc1(t)\n",
    "        t = F.relu(t)\n",
    "        \n",
    "        # 5) fc2\n",
    "        t = self.fc2(t)\n",
    "        t = F.relu(t)\n",
    "\n",
    "        # 6) out \n",
    "        t = self.out(t)\n",
    "#         t.softmax(t, dim=1) # change to percentages. Different activation function\n",
    "        \n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_num_correct(preds, labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum() # number of correct guesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LOAD\n",
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root = './data/FashionMNIST',\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = transforms.Compose([ # This is the transform. Easy. \n",
    "        transforms.ToTensor() # TRANSFORM\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "images, labels = batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network()\n",
    "preds = network(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = F.cross_entropy(preds, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3082938194274902"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.item() # summed differences of predictions from labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0) cl: 8.872130393981934\n",
      "10) cl: 0.12297342717647552\n",
      "20) cl: 2.8746678829193115\n",
      "30) cl: 4.868659973144531\n",
      "40) cl: 5.863234043121338\n",
      "50) cl: 2.876112937927246\n",
      "60) cl: 4.870473384857178\n",
      "70) cl: 2.8767216205596924\n",
      "80) cl: 0.8737058043479919\n",
      "90) cl: 8.871868133544922\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(preds)):\n",
    "    loss_i = abs(preds[i].max().item() - labels[i])\n",
    "    if i%10==0:\n",
    "        print(\"{}) cl: {}\".format(i, loss_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# we have our loss. Now, what about gradients?\n",
    "print(network.conv1.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-1.0213e-03, -6.0217e-04, -3.5090e-04, -6.6514e-04, -1.6521e-03],\n",
      "          [-6.5744e-04, -3.2261e-04, -3.8479e-04, -7.4669e-04, -1.7587e-03],\n",
      "          [-7.6745e-04, -3.9491e-04, -4.6891e-04, -1.0875e-03, -1.7523e-03],\n",
      "          [-8.4279e-04, -4.1597e-04, -2.8424e-04, -1.0495e-03, -1.8093e-03],\n",
      "          [-3.8704e-04, -5.6582e-04, -6.3969e-04, -1.5271e-03, -1.8251e-03]]],\n",
      "\n",
      "\n",
      "        [[[-3.5149e-04, -3.5799e-05,  5.0507e-06, -4.1301e-06,  8.2016e-07],\n",
      "          [-2.0818e-04,  7.2200e-06, -2.9325e-06, -6.7616e-06,  2.4076e-06],\n",
      "          [-1.5251e-04,  1.3392e-06, -5.3205e-06, -1.1740e-05,  1.5879e-05],\n",
      "          [-1.4451e-04, -4.7039e-06,  4.7099e-06,  3.9046e-05, -1.1307e-04],\n",
      "          [-7.1386e-05,  1.0393e-05,  2.0262e-05, -3.8235e-05, -4.3980e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 1.1552e-03,  2.9186e-04, -4.0394e-04, -2.4013e-04, -6.2014e-04],\n",
      "          [ 8.4670e-04,  4.1739e-04, -3.2125e-04, -5.0695e-04, -5.2845e-04],\n",
      "          [ 8.3865e-04,  4.0994e-04, -6.7180e-05, -7.4328e-04, -6.4395e-04],\n",
      "          [ 7.1835e-04,  3.0377e-04, -4.7847e-04, -1.0449e-03, -9.7537e-04],\n",
      "          [ 6.3684e-04,  2.8961e-04, -1.4614e-04, -4.9833e-04, -3.9612e-04]]],\n",
      "\n",
      "\n",
      "        [[[-1.1466e-03, -1.6063e-03, -9.6183e-04, -1.0512e-03, -8.0012e-04],\n",
      "          [-8.8428e-04, -1.0312e-03, -4.3421e-04, -5.8017e-04, -1.7725e-04],\n",
      "          [-5.1873e-04, -4.2484e-04, -1.7345e-04, -2.4630e-04, -2.2364e-04],\n",
      "          [-2.6399e-04, -2.4210e-04, -1.5313e-04, -4.7003e-04, -3.9259e-04],\n",
      "          [-4.9983e-04, -2.9800e-04, -3.2614e-04, -3.4646e-04, -3.5304e-04]]],\n",
      "\n",
      "\n",
      "        [[[-1.7336e-03, -1.8087e-03, -1.1233e-03, -3.0626e-04,  2.9020e-04],\n",
      "          [-1.8277e-03, -2.1587e-03, -1.3235e-03, -3.5288e-04,  4.6005e-04],\n",
      "          [-1.7326e-03, -1.8378e-03, -1.3486e-03, -2.5533e-04,  4.5392e-04],\n",
      "          [-1.7823e-03, -1.7867e-03, -1.1897e-03, -4.0399e-04,  3.9973e-04],\n",
      "          [-1.8092e-03, -1.9503e-03, -1.1083e-03, -7.0420e-04,  1.5267e-04]]],\n",
      "\n",
      "\n",
      "        [[[-2.6002e-04, -1.6291e-04,  1.0037e-04,  2.2196e-04,  3.1543e-04],\n",
      "          [-4.1467e-04, -6.3456e-04, -3.4271e-04,  1.3485e-04, -2.7440e-04],\n",
      "          [-4.4396e-04, -7.4314e-04, -2.8346e-04, -2.4245e-04, -2.2706e-04],\n",
      "          [-3.1603e-04, -6.5298e-04, -2.1662e-04,  1.8484e-04, -6.3802e-04],\n",
      "          [-8.2698e-04, -8.7707e-04, -3.8765e-04, -2.8384e-04, -1.2661e-03]]]])\n"
     ]
    }
   ],
   "source": [
    "print(network.conv1.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Nice backprop is done, and gradients for each weight tensor in the grad attribute have been updated\n",
    "# step 4 done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update The Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# update weights according to gradients\n",
    "# we need an optimizer to do this. \n",
    "optimizer = optim.Adam(network.parameters(), lr=.01) # need to pass the actual weights we want updated. \n",
    "# these weights live in the network's parameters instance. \n",
    "# lr is the learning rate. Dictates the size of the \"leap\" each learning step takes \n",
    "\n",
    "get_num_correct(preds, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer.step() # take a step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = network(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_num_correct(preds, labels)\n",
    "loss = F.cross_entropy(preds, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2794179916381836"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "optimizer.step()\n",
    "preds = network(images)\n",
    "loss = F.cross_entropy(preds, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.234861373901367"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.item() # bada bing bada boom. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_correct(preds, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:50\n",
      "\t\tloss: 2.303581476211548\n",
      "\t\tnum_correct: 11\n",
      "i:100\n",
      "\t\tloss: 2.2861835956573486\n",
      "\t\tnum_correct: 11\n",
      "i:150\n",
      "\t\tloss: 2.261082410812378\n",
      "\t\tnum_correct: 12\n",
      "i:200\n",
      "\t\tloss: 2.2494986057281494\n",
      "\t\tnum_correct: 12\n",
      "i:250\n",
      "\t\tloss: 2.198390483856201\n",
      "\t\tnum_correct: 16\n",
      "i:300\n",
      "\t\tloss: 2.1417696475982666\n",
      "\t\tnum_correct: 21\n",
      "i:350\n",
      "\t\tloss: 2.114436388015747\n",
      "\t\tnum_correct: 33\n",
      "i:400\n",
      "\t\tloss: 2.068920373916626\n",
      "\t\tnum_correct: 37\n",
      "i:450\n",
      "\t\tloss: 1.9670729637145996\n",
      "\t\tnum_correct: 36\n",
      "i:500\n",
      "\t\tloss: 1.8496395349502563\n",
      "\t\tnum_correct: 29\n"
     ]
    }
   ],
   "source": [
    "# now in a loop. \n",
    "i = 0\n",
    "network = Network()\n",
    "optimizer = optim.Adam(network.parameters(), lr=.01) # need to pass the actual weights we want updated. \n",
    "\n",
    "preds = network(images)\n",
    "loss = F.cross_entropy(preds, labels)\n",
    "\n",
    "while i < 500:\n",
    "    i+=1\n",
    "    if i%50==0:\n",
    "        preds = network(images)\n",
    "        loss = F.cross_entropy(preds, labels)\n",
    "        print(\"i:{}\\n\\t\\tloss: {}\".format(i, loss.item()))\n",
    "        print(\"\\t\\tnum_correct: {}\".format(get_num_correct(preds, labels)))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "# here, we jump over a local minimum a few times. check out how num_correct goes up and down\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now let's try it with a bunch of different batches - this is what is actually relevant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epoch Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 total_correct: tensor(47745) loss: 324.13429021835327\n"
     ]
    }
   ],
   "source": [
    "network = Network()\n",
    "optimizer = optim.Adam(network.parameters(), lr=.01) # need to pass the actual weights we want updated. \n",
    "\n",
    "total_loss = 0\n",
    "total_correct = 0\n",
    "\n",
    "for batch in train_loader:\n",
    "    images, labels = batch\n",
    "    preds = network(images)\n",
    "    loss = F.cross_entropy(preds, labels)\n",
    "\n",
    "    total_loss += loss.item()\n",
    "    total_correct += get_num_correct(preds, labels)\n",
    "    \n",
    "    optimizer.zero_grad() # clean up the gradients before calculating new ones with each step\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(\"epoch:\", 0, \"total_correct:\", total_correct, \"loss:\", total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79575"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_correct.item()/len(train_set) # not bad, not bad at all\n",
    "# changing the batch size means that we change the number of steps we can take via the learning step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
